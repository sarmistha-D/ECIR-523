{"cells":[{"cell_type":"code","execution_count":null,"id":"c0d7acf1-8591-463e-b815-66258f83ed7f","metadata":{"id":"c0d7acf1-8591-463e-b815-66258f83ed7f"},"outputs":[],"source":["from datasets import load_dataset\n","import torch\n","from PIL import Image"]},{"cell_type":"code","execution_count":null,"id":"a0ab712f-c370-4162-afe2-adcacb2a9cfd","metadata":{"id":"a0ab712f-c370-4162-afe2-adcacb2a9cfd"},"outputs":[],"source":["print(\"++++++Reading the Dataset++++++++++\")\n","dataset = load_dataset(\"Anonymous/Final_idiom_all\",split='train') #<======== Please change the dataset in csv file."]},{"cell_type":"code","execution_count":null,"id":"06d38808-f5d0-4069-8e03-a83a3e17ab66","metadata":{"id":"06d38808-f5d0-4069-8e03-a83a3e17ab66"},"outputs":[],"source":["from huggingface_hub import login\n","\n","login(token='Anonymous_xyzkajwjewkncjqnkj') #<======= PlaceHolder"]},{"cell_type":"code","execution_count":null,"id":"b4cec1c6-af32-485f-a7a9-f8adf6757c6f","metadata":{"id":"b4cec1c6-af32-485f-a7a9-f8adf6757c6f"},"outputs":[],"source":["system_message='''You are an polyglot, who are having exceptional linguistic and cultural domain knowledge. Also, you are an native speaker of hindi, bengali and thai.'''"]},{"cell_type":"code","execution_count":null,"id":"e6e8e226-f1af-44e4-9ec6-e84ba7484747","metadata":{"id":"e6e8e226-f1af-44e4-9ec6-e84ba7484747"},"outputs":[],"source":["def format_data(sample):\n","    return {\n","    \"messages\":[\n","        {\n","            \"role\": \"system\",\n","            \"content\": [\n","                {\n","                    \"type\": \"text\",\n","                    \"text\": system_message\n","                }\n","            ],\n","        },\n","        {\n","            \"role\": \"user\",\n","            \"content\": [\n","                {\n","                    \"type\": \"text\",\n","                    \"text\": sample['Actual idiom'],\n","                },\n","                {\n","                    \"type\": \"image\",\n","                    \"image\": sample[\"image\"],\n","                },\n","\n","            ],\n","        },\n","        {\n","            \"role\": \"assistant\",\n","            \"content\": [\n","                {\n","                    \"type\": \"text\",\n","                    \"text\": sample[\"Descriptive Meaning(Human Annotation)\"]\n","                }\n","            ],\n","        },\n","    ],\n","    }"]},{"cell_type":"code","execution_count":null,"id":"9ae39433-0f80-4ad4-bba1-c4ba19495af8","metadata":{"id":"9ae39433-0f80-4ad4-bba1-c4ba19495af8"},"outputs":[],"source":["def process_vision_info(messages: list[dict]) -> list[Image.Image]:\n","    image_inputs = []\n","    # Iterate through each conversation\n","    for msg in messages:\n","        # Get content (ensure it's a list)\n","        content = msg.get(\"content\", [])\n","        if not isinstance(content, list):\n","            content = [content]\n","\n","        # Check each content element for images\n","        for element in content:\n","            if isinstance(element, dict) and (\n","                \"image\" in element or element.get(\"type\") == \"image\"\n","            ):\n","                # Get the image and convert to RGB\n","                if \"image\" in element:\n","                    image = element[\"image\"]\n","                else:\n","                    image = element\n","                image_inputs.append(image.convert(\"RGB\"))\n","    return image_inputs"]},{"cell_type":"code","execution_count":null,"id":"5d62db64-f863-451a-b8f5-29e286306707","metadata":{"id":"5d62db64-f863-451a-b8f5-29e286306707"},"outputs":[],"source":["print(dataset)"]},{"cell_type":"code","execution_count":null,"id":"98e26f04-7523-48d2-b85f-c8737831eb85","metadata":{"id":"98e26f04-7523-48d2-b85f-c8737831eb85"},"outputs":[],"source":["# print(dataset[3132])\n","# print(len(dataset))"]},{"cell_type":"code","execution_count":null,"id":"cba40892-aa71-4186-9386-06bc31e0b481","metadata":{"id":"cba40892-aa71-4186-9386-06bc31e0b481"},"outputs":[],"source":["print(\"++++++Seperating the Dataset on Lingual Basis++++++++++\")\n","dataset_hindi = dataset.select(range(0,1277))\n","dataset_thai = dataset.select(range(1382,3133))\n","bengali_indicies = list(range(1277,1382))+list(range(3133,3533))\n","dataset_bengali= dataset.select(bengali_indicies)"]},{"cell_type":"code","execution_count":null,"id":"b3f3186b-4da4-44b5-98d9-cb35f41b2d51","metadata":{"id":"b3f3186b-4da4-44b5-98d9-cb35f41b2d51"},"outputs":[],"source":["from datasets import Dataset,concatenate_datasets"]},{"cell_type":"code","execution_count":null,"id":"6b9fc488-0948-4e31-8beb-70ccaf1838f5","metadata":{"id":"6b9fc488-0948-4e31-8beb-70ccaf1838f5"},"outputs":[],"source":["def split_dataset(dataset1):\n","    train_testvalid = dataset1.train_test_split(test_size=0.3, seed=42)\n","    train_dataset = train_testvalid['train']\n","    temp_dataset = train_testvalid['test']\n","\n","    # Step 2: Split the remaining 30% into 2/3 (validation) and 1/3 (test)\n","    # 2/3 of 30% = 20%, 1/3 of 30% = 10%\n","    val_test = temp_dataset.train_test_split(test_size=2/3, seed=42)\n","    val_dataset = val_test['train']    # 20%\n","    test_dataset = val_test['test']    # 10%\n","\n","    return train_dataset, val_dataset, test_dataset"]},{"cell_type":"code","execution_count":null,"id":"008173e7-07f3-44f8-a90d-81ec70f3a187","metadata":{"id":"008173e7-07f3-44f8-a90d-81ec70f3a187"},"outputs":[],"source":["print(\"++++++Splitting the Dataset and Merging++++++++++\")\n","train_dataset_hindi, val_dataset_hindi, test_dataset_hindi = split_dataset(dataset_hindi)\n","train_dataset_thai, val_dataset_thai, test_dataset_thai = split_dataset(dataset_thai)\n","train_dataset_bengali, val_dataset_bengali, test_dataset_bengali = split_dataset(dataset_bengali)\n","\n","\n","#Merging the dataset\n","train_dataset_final = concatenate_datasets([train_dataset_hindi,train_dataset_thai,train_dataset_bengali])\n","val_dataset_final = concatenate_datasets([val_dataset_hindi,val_dataset_thai,val_dataset_bengali])\n","test_dataset_final = concatenate_datasets([test_dataset_hindi,test_dataset_thai,test_dataset_bengali])\n","\n","print(len(train_dataset_final),len(val_dataset_final),len(test_dataset_final))\n","\n","\n","# test_dataset_final.save_to_disk(\"test_dataset_Gemma3\")"]},{"cell_type":"code","execution_count":null,"id":"6c278b2e-0698-4ed3-a873-3f34b67380f7","metadata":{"id":"6c278b2e-0698-4ed3-a873-3f34b67380f7"},"outputs":[],"source":["print(\"++++++Converting the Dataset to JSON format++++++++++\")\n","train_dataset = [format_data(sample) for sample in train_dataset_final]\n","eval_dataset = [format_data(sample) for sample in val_dataset_final]\n","test_dataset = [format_data(sample) for sample in test_dataset_final]"]},{"cell_type":"code","execution_count":null,"id":"fbba9081-ca0e-4455-bfc0-c8a1bce4cc33","metadata":{"id":"fbba9081-ca0e-4455-bfc0-c8a1bce4cc33"},"outputs":[],"source":["print(train_dataset[2000])"]},{"cell_type":"code","execution_count":null,"id":"3d36c699-0bac-46ee-9dd2-b065081f3a99","metadata":{"id":"3d36c699-0bac-46ee-9dd2-b065081f3a99"},"outputs":[],"source":["import torch\n","from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n","\n","print(\"+++++++++++Loading Model+++++++++++\")\n","# Hugging Face model id\n","model_id = \"google/gemma-3-12b-pt\" # or `google/gemma-3-12b-pt`, `google/gemma-3-27-pt`\n","\n","# Check if GPU benefits from bfloat16\n","if torch.cuda.get_device_capability()[0] < 8:\n","    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n","\n","# Define model init arguments\n","model_kwargs = dict(\n","    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU\n","    torch_dtype=torch.bfloat16, # What torch dtype to use, defaults to auto\n","    device_map=\"auto\", # Let torch decide how to load the model\n",")\n","\n","# BitsAndBytesConfig int-4 config\n","model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n","    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",")\n","\n","# Load model and tokenizer\n","model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n","processor = AutoProcessor.from_pretrained(\"google/gemma-3-12b-it\")"]},{"cell_type":"code","execution_count":null,"id":"afd3eadf-2261-4f73-8014-88a852ed8fe3","metadata":{"id":"afd3eadf-2261-4f73-8014-88a852ed8fe3"},"outputs":[],"source":["print(\"++++++Configuring LoRA and peft++++++++++\")\n","from peft import LoraConfig, get_peft_model\n","\n","# Configure LoRA\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    r=16,\n","    bias=\"none\",\n","    target_modules=\"all-linear\",\n","    task_type=\"CAUSAL_LM\",\n","    modules_to_save=[\n","        \"lm_head\",\n","        \"embed_tokens\",\n","    ],\n",")"]},{"cell_type":"code","execution_count":null,"id":"d542e73b-bd14-4b7d-b74e-2f57b6803e71","metadata":{"id":"d542e73b-bd14-4b7d-b74e-2f57b6803e71"},"outputs":[],"source":["from trl import SFTConfig\n","\n","args = SFTConfig(\n","    output_dir=\"Gemma_3_Idiom_VL\",     # directory to save and repository id\n","    num_train_epochs=3,                         # number of training epochs\n","    per_device_train_batch_size=4,              # batch size per device during training\n","    gradient_accumulation_steps=8,              # number of steps before performing a backward/update pass\n","    gradient_checkpointing=True,                # use gradient checkpointing to save memory\n","    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n","    logging_steps=10,                            # log every 5 steps\n","    save_strategy=\"steps\",\n","    save_steps=20,                    # save checkpoint every epoch\n","    learning_rate=2e-4,                         # learning rate, based on QLoRA paper\n","    bf16=True,                                  # use bfloat16 precision\n","    max_grad_norm=0.3,                          # max gradient norm based on QLoRA paper\n","    warmup_ratio=0.03,                          # warmup ratio based on QLoRA paper\n","    lr_scheduler_type=\"constant\",               # use constant learning rate scheduler\n","    push_to_hub=True,                           # push model to hub\n","    report_to=\"none\",                    # report metrics to tensorboard\n","    gradient_checkpointing_kwargs={\n","        \"use_reentrant\": False\n","    },  # use reentrant checkpointing\n","    dataset_text_field=\"\",                      # need a dummy field for collator\n","    dataset_kwargs={\"skip_prepare_dataset\": True},  # important for collator\n",")\n","args.remove_unused_columns = False # important for collator"]},{"cell_type":"code","execution_count":null,"id":"c8896a59-d7c9-4b90-99a0-b9ae7d82ef07","metadata":{"id":"c8896a59-d7c9-4b90-99a0-b9ae7d82ef07"},"outputs":[],"source":["# Create a data collator to encode text and image pairs\n","def collate_fn(examples):\n","    texts = []\n","    images = []\n","    for example in examples:\n","        # print(examples)\n","        image_inputs = process_vision_info(example[\"messages\"])\n","        text = processor.apply_chat_template(\n","            example[\"messages\"], add_generation_prompt=False, tokenize=False\n","        )\n","        texts.append(text.strip())\n","        images.append(image_inputs)\n","\n","    # Tokenize the texts and process the images\n","    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n","\n","    # The labels are the input_ids, and we mask the padding tokens and image tokens in the loss computation\n","    labels = batch[\"input_ids\"].clone()\n","\n","    # Mask image tokens\n","    image_token_id = [\n","        processor.tokenizer.convert_tokens_to_ids(\n","            processor.tokenizer.special_tokens_map[\"boi_token\"]\n","        )\n","    ]\n","    # Mask tokens for not being used in the loss computation\n","    labels[labels == processor.tokenizer.pad_token_id] = -100\n","    labels[labels == image_token_id] = -100\n","    labels[labels == 262144] = -100\n","\n","    batch[\"labels\"] = labels\n","    return batch\n"]},{"cell_type":"code","execution_count":null,"id":"2843f3eb-221a-4b04-adfe-78a34660d392","metadata":{"id":"2843f3eb-221a-4b04-adfe-78a34660d392"},"outputs":[],"source":["from trl import SFTTrainer\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    peft_config=peft_config,\n","    processing_class=processor,\n","    data_collator=collate_fn,\n",")"]},{"cell_type":"code","execution_count":null,"id":"120d43b8-b7ca-4e69-8eee-d164e77a8d76","metadata":{"id":"120d43b8-b7ca-4e69-8eee-d164e77a8d76"},"outputs":[],"source":["print(\"++++++Starting the training++++++++++\")\n","trainer.train()"]},{"cell_type":"code","execution_count":null,"id":"2511861e-72aa-44fe-994e-49155a2f0ee6","metadata":{"id":"2511861e-72aa-44fe-994e-49155a2f0ee6"},"outputs":[],"source":["print(\"++++++Saving the Model++++++++++\")\n","trainer.save_model(args.output_dir)\n"]},{"cell_type":"code","execution_count":null,"id":"77a064a3-b275-4a63-a063-42e0bb51fabc","metadata":{"id":"77a064a3-b275-4a63-a063-42e0bb51fabc"},"outputs":[],"source":["# free the memory again\n","del model\n","del trainer\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"id":"b8751de4-4ab6-4935-8446-23e6aed9e772","metadata":{"id":"b8751de4-4ab6-4935-8446-23e6aed9e772"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python vvv","language":"python","name":"vvv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.13.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}