{"cells":[{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from datasets import load_from_disk\n","from tqdm import tqdm\n","from bert_score import score\n","import pandas as pd\n","import evaluate\n","from peft import PeftModel\n","import os\n","\n","from transformers import Idefics3ForConditionalGeneration, AutoProcessor\n","\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Multi-Model HyperMoE\n","class HyperMoE(nn.Module):\n","    def __init__(self, embed_dim, num_experts=4, hidden_dim=1024):\n","        super().__init__()\n","        self.experts = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(embed_dim, hidden_dim),\n","                nn.ReLU(),\n","                nn.Linear(hidden_dim, embed_dim)\n","            ) for _ in range(num_experts)\n","        ])\n","        self.gate = nn.Linear(embed_dim, num_experts)\n","\n","    def forward(self, x):\n","        # x: (B, embed_dim)\n","        weights = F.softmax(self.gate(x), dim=-1)  # (B, num_experts)\n","        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)  # (B, num_experts, embed_dim)\n","        expert_mean = expert_outputs.mean(dim=1, keepdim=True)  # (B, 1, embed_dim)\n","        enhanced = expert_outputs + 0.1 * (expert_mean - expert_outputs)  # (B, num_experts, embed_dim)\n","        fused = torch.einsum(\"be,bed->bd\", weights, enhanced)  # (B, embed_dim)\n","        return fused\n","\n","# --- Load base model and processor ---\n","model_id = \"HuggingFaceTB/SmolVLM-Instruct\"\n","processor = AutoProcessor.from_pretrained(model_id)\n","model = Idefics3ForConditionalGeneration.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",\n","    torch_dtype=torch.bfloat16,\n","    _attn_implementation=\"flash_attention_2\"\n",")\n","# Load LoRA adapter\n","model = PeftModel.from_pretrained(model, \"Anonymous/SmolVLM_Idiom_VL\", adapter_name=\"default\")\n","model.gradient_checkpointing_enable()\n","\n","\n","# Determine embedding dimension for hypermoe\n","embed_dim = model.base_model.config.text_config.hidden_size\n","\n","# Create multiple HyperMoE modules\n","num_moe = 3\n","moe_modules = nn.ModuleList([HyperMoE(embed_dim) for _ in range(num_moe)]).to(DEVICE)\n","\n","def multi_model_hypermoe_fusion(features: torch.Tensor) -> torch.Tensor:\n","    \"\"\"\n","    Pass the features (batch of [embed_dim] vectors) through multiple HyperMoE modules and fuse their outputs.\n","    \"\"\"\n","    # features shape: (B, embed_dim)\n","    fused_outputs = [moe(features) for moe in moe_modules]  # list of (B, embed_dim) from each HyperMoE\n","    stacked = torch.stack(fused_outputs, dim=1)             # shape (B, num_moe, embed_dim)\n","    final_fusion = stacked.mean(dim=1)                      # simple averaging across the num_moe outputs -> (B, embed_dim)\n","    return final_fusion\n","\n","# Hook to modify text model output (the combined vision-language model output)\n","def model_hook(module, inputs, outputs):\n","    if hasattr(outputs, \"last_hidden_state\"):\n","        h = outputs.last_hidden_state\n","    elif isinstance(outputs, tuple) and isinstance(outputs[0], torch.Tensor):\n","        h = outputs[0]\n","    else:\n","        return outputs\n","    cls = h[:, 0, :]\n","    fused = multi_model_hypermoe_fusion(cls)\n","    h[:, 0, :] = fused\n","    return outputs.__class__(**{**outputs.__dict__, \"last_hidden_state\": h})\n","\n","\n","# Hook to modify image encoder output (after connector) before merging into text embeddings\n","def connector_hook(module, inputs, output):\n","    \"\"\"\n","    Forward hook for the vision-to-text connector output. Modifies the first token of image features using HyperMoE.\n","    \"\"\"\n","    # output from connector is expected to be a tensor of shape (N, image_seq_len, embed_dim)\n","    if isinstance(output, torch.Tensor) and output.dim() == 3:\n","        image_features = output  # shape (N, seq_len_img, embed_dim)\n","        # Apply HyperMoE to the first token of each image feature sequence (e.g., CLS token if present)\n","        cls_image_embed = image_features[:, 0, :]           # (N, embed_dim)\n","        fused_image_embed = multi_model_hypermoe_fusion(cls_image_embed)  # fuse image CLS embeddings\n","        image_features[:, 0, :] = fused_image_embed         # replace first token features with fused embeddings\n","        return image_features\n","    # If output is not a tensor or not 3D, return it unchanged\n","    return output\n","\n","# attach text-fusion hook\n","model.base_model.model.register_forward_hook(model_hook)\n","\n","# dynamically find and hook the vision encoder\n","from transformers.models.idefics3.modeling_idefics3 import Idefics3VisionTransformer\n","vision_module = next(\n","    m for m in model.base_model.model.modules()\n","    if isinstance(m, Idefics3VisionTransformer)\n",")\n","vision_module.register_forward_hook(connector_hook)\n","\n","# Load fine-tuning dataset\n","data = load_from_disk(\"idiom_vaibhav\")\n","train_dataset = data[\"train\"]\n","val_dataset = data[\"validation\"]\n","\n","# System message for prompting\n","system_message = (\n","    \"You are a polyglot assistant with exceptional linguistic and cultural knowledge. \"\n","    \"You are a native speaker of Hindi, Bengali, and Thai.\"\n",")\n","\n","# Collate function for DataLoader\n","def collate_fn(examples):\n","    images = [ex[\"image\"] for ex in examples]\n","    prompts = []\n","    for ex in examples:\n","        system_msg = {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]}\n","        user_msg = {\"role\": \"user\", \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": ex[\"Actual idiom\"]}]}\n","        assistant_msg = {\"role\": \"assistant\", \"content\": [{\"type\": \"text\", \"text\": ex[\"Descriptive Meaning(Human Annotation)\"]}]}\n","        # Create prompt including answer\n","        messages = [system_msg, user_msg, assistant_msg]\n","        prompt = processor.apply_chat_template(messages, add_generation_prompt=False)\n","        prompts.append(prompt)\n","    # Process inputs with padding\n","    inputs = processor(text=prompts, images=[[img] for img in images], padding=True, return_tensors=\"pt\")\n","    # Prepare labels by cloning input_ids\n","    labels = inputs.input_ids.clone()\n","    # Mask padding and image tokens\n","    labels[labels == processor.tokenizer.pad_token_id] = -100\n","    labels[labels == model.config.image_token_id] = -100\n","    inputs[\"labels\"] = labels\n","    return inputs\n","\n","# DataLoaders\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n","val_loader   = torch.utils.data.DataLoader(val_dataset,   batch_size=4, shuffle=False, collate_fn=collate_fn)\n","\n","# Move model and MoE modules to device\n","model.to(DEVICE)\n","moe_modules.to(DEVICE)\n","\n","# Ensure all parameters trainable (full fine-tuning)\n","model.train()\n","for p in model.parameters():\n","    p.requires_grad = True\n","moe_modules.train()\n","\n","optimizer = torch.optim.AdamW(list(model.parameters()) + list(moe_modules.parameters()), lr=2e-5)\n","\n","# --- Training loop ---\n","from torch.cuda.amp import autocast\n","\n","for epoch in range(1):  # 1 epoch\n","    model.train()\n","    epoch_loss = 0.0\n","    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n","        optimizer.zero_grad()\n","        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n","        with autocast(dtype=torch.bfloat16):\n","            outputs = model(\n","                input_ids=batch[\"input_ids\"],\n","                attention_mask=batch.get(\"attention_mask\", None),\n","                pixel_values=batch.get(\"pixel_values\", None),\n","                pixel_attention_mask=batch.get(\"pixel_attention_mask\", None),\n","                labels=batch[\"labels\"]\n","            )\n","            loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","    print(f\"Epoch {epoch+1} completed. Avg Loss: {epoch_loss/len(train_loader):.4f}\")\n"],"metadata":{"id":"a7EFZ5GEXyQR"},"id":"a7EFZ5GEXyQR","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}