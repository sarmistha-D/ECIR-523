{"cells":[{"cell_type":"code","source":["# ---- Full Fine-tuning with HyperMoE integration ----\n","\n","from datasets import load_dataset, concatenate_datasets\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from PIL import Image\n","from huggingface_hub import login\n","from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n","from peft import LoraConfig, get_peft_model\n","from trl import SFTConfig, SFTTrainer\n","import wandb\n","\n","# Login to Hugging Face Hub\n","login(token='Anonymous_xvueicgiuewuhouhqwhixh')\n","\n","# Define system message used in dataset formatting\n","system_message = '''You are an polyglot, who are having exceptional linguistic and cultural domain knowledge. Also, you are a native speaker of Hindi, Bengali and Thai.'''\n","\n","# Function to convert samples to conversation format\n","def format_data(sample):\n","    return {\n","        \"messages\": [\n","            {\n","                \"role\": \"system\",\n","                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"text\", \"text\": sample['Actual idiom']},\n","                    {\"type\": \"image\", \"image\": sample[\"image\"]},\n","                ],\n","            },\n","            {\n","                \"role\": \"assistant\",\n","                \"content\": [{\"type\": \"text\", \"text\": sample[\"Descriptive Meaning(Human Annotation)\"]}],\n","            },\n","        ],\n","    }\n","\n","# Helper to extract image objects from messages\n","def process_vision_info(messages: list[dict]) -> list[Image.Image]:\n","    image_inputs = []\n","    for msg in messages:\n","        content = msg.get(\"content\", [])\n","        if not isinstance(content, list):\n","            content = [content]\n","        for element in content:\n","            if isinstance(element, dict) and (\"image\" in element or element.get(\"type\") == \"image\"):\n","                image = element.get(\"image\", element)\n","                image_inputs.append(image.convert(\"RGB\"))\n","    return image_inputs\n","\n","print(\"++++++Reading the Dataset++++++++++\")\n","dataset = load_dataset(\"Anonymous/Final_idiom_all\", split='train')\n","\n","print(\"++++++Separating the Dataset on Lingual Basis++++++++++\")\n","dataset_hindi = dataset.select(range(0,1277))\n","dataset_thai = dataset.select(range(1382,3133))\n","bengali_indices = list(range(1277,1382)) + list(range(3133,3533))\n","dataset_bengali = dataset.select(bengali_indices)\n","\n","def split_dataset(dataset1):\n","    train_testvalid = dataset1.train_test_split(test_size=0.3, seed=42)\n","    train_dataset = train_testvalid['train']\n","    temp_dataset = train_testvalid['test']\n","    val_test = temp_dataset.train_test_split(test_size=2/3, seed=42)\n","    val_dataset = val_test['train']    # 20%\n","    test_dataset = val_test['test']    # 10%\n","    return train_dataset, val_dataset, test_dataset\n","\n","print(\"++++++Splitting the Dataset and Merging++++++++++\")\n","train_dataset_hindi, val_dataset_hindi, test_dataset_hindi = split_dataset(dataset_hindi)\n","train_dataset_thai, val_dataset_thai, test_dataset_thai = split_dataset(dataset_thai)\n","train_dataset_bengali, val_dataset_bengali, test_dataset_bengali = split_dataset(dataset_bengali)\n","\n","train_dataset_final = concatenate_datasets([train_dataset_hindi, train_dataset_thai, train_dataset_bengali])\n","val_dataset_final = concatenate_datasets([val_dataset_hindi, val_dataset_thai, val_dataset_bengali])\n","test_dataset_final = concatenate_datasets([test_dataset_hindi, test_dataset_thai, test_dataset_bengali])\n","\n","print(f\"Train size: {len(train_dataset_final)}, Val size: {len(val_dataset_final)}, Test size: {len(test_dataset_final)}\")\n","\n","print(\"++++++Converting the Dataset to JSON format++++++++++\")\n","train_dataset = [format_data(sample) for sample in train_dataset_final]\n","eval_dataset = [format_data(sample) for sample in val_dataset_final]\n","test_dataset = [format_data(sample) for sample in test_dataset_final]\n","\n","# --------- Model & Processor Loading ---------\n","print(\"+++++++++++Loading Model+++++++++++\")\n","model_id = \"google/gemma-3-12b-pt\"\n","\n","if torch.cuda.get_device_capability()[0] < 8:\n","    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n","\n","model_kwargs = dict(\n","    attn_implementation=\"eager\",\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n","    quantization_config=BitsAndBytesConfig(\n","        load_in_4bit=True,\n","        bnb_4bit_use_double_quant=True,\n","        bnb_4bit_quant_type=\"nf4\",\n","        bnb_4bit_compute_dtype=torch.bfloat16,\n","        bnb_4bit_quant_storage=torch.bfloat16,\n","    ),\n",")\n","\n","model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n","processor = AutoProcessor.from_pretrained(\"google/gemma-3-12b-it\")\n","\n","# --------- HyperMoE Logic ---------\n","class HyperMoE(nn.Module):\n","    def __init__(self, embed_dim, num_experts=4, hidden_dim=1024):\n","        super().__init__()\n","        self.experts = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(embed_dim, hidden_dim),\n","                nn.ReLU(),\n","                nn.Linear(hidden_dim, embed_dim)\n","            ) for _ in range(num_experts)\n","        ])\n","        self.gate = nn.Linear(embed_dim, num_experts)\n","\n","    def forward(self, x):\n","        weights = F.softmax(self.gate(x), dim=-1)\n","        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)\n","        expert_mean = expert_outputs.mean(dim=1, keepdim=True)\n","        enhanced = expert_outputs + 0.1 * (expert_mean - expert_outputs)\n","        fused = torch.einsum(\"be,bed->bd\", weights, enhanced)\n","        return fused\n","\n","# Get embedding dimension from model config dynamically\n","def get_embed_dim(model):\n","    if hasattr(model.base_model.config, \"text_config\") and hasattr(model.base_model.config.text_config, \"hidden_size\"):\n","        return model.base_model.config.text_config.hidden_size\n","    elif hasattr(model.base_model.config, \"hidden_size\"):\n","        return model.base_model.config.hidden_size\n","    else:\n","        raise ValueError(\"Cannot find embedding dimension in the model config\")\n","\n","embed_dim = get_embed_dim(model)\n","num_moe = 3\n","moe_modules = nn.ModuleList([HyperMoE(embed_dim) for _ in range(num_moe)]).to(\"cuda\")\n","\n","def multi_model_hypermoe_fusion(features: torch.Tensor) -> torch.Tensor:\n","    fused_outputs = [moe(features) for moe in moe_modules]  # list of (B, embed_dim)\n","    stacked = torch.stack(fused_outputs, dim=1)             # (B, num_moe, embed_dim)\n","    final_fusion = stacked.mean(dim=1)                      # average to (B, embed_dim)\n","    return final_fusion\n","\n","def model_hook(module, inputs, outputs):\n","    if hasattr(outputs, \"last_hidden_state\"):\n","        h = outputs.last_hidden_state\n","    elif isinstance(outputs, tuple) and isinstance(outputs[0], torch.Tensor):\n","        h = outputs[0]\n","    else:\n","        return outputs\n","    cls = h[:, 0, :]\n","    fused = multi_model_hypermoe_fusion(cls)\n","    # Create a new tensor instead of modifying in-place\n","    h_new = h.clone()\n","    h_new[:, 0, :] = fused\n","    # Return a copy of the same class as outputs, but with the modified hidden state\n","    return outputs.__class__(**{**outputs.__dict__, \"last_hidden_state\": h_new})\n","\n","def connector_hook(module, inputs, output):\n","    if isinstance(output, torch.Tensor) and output.dim() == 3:\n","        image_features = output\n","        cls_image_embed = image_features[:, 0, :]\n","        fused_image_embed = multi_model_hypermoe_fusion(cls_image_embed)\n","        image_features_new = image_features.clone()\n","        image_features_new[:, 0, :] = fused_image_embed\n","        return image_features_new\n","    return output\n","\n","\n","# Register hooks to the model\n","model.base_model.model.register_forward_hook(model_hook)\n","\n","# Try to find the vision module to register connector hook\n","# NOTE: For Gemma3 you may need to verify the exact class name of vision encoder\n","vision_module = None\n","for m in model.base_model.model.modules():\n","    # Adapt this to Gemma3â€™s vision transformer class if different\n","    if m.__class__.__name__ == \"Idefics3VisionTransformer\":\n","        vision_module = m\n","        break\n","\n","if vision_module is not None:\n","    vision_module.register_forward_hook(connector_hook)\n","else:\n","    print(\"Warning: Vision module hook not registered; adjust class name for Gemma3 vision encoder.\")\n","\n","# --------- LoRA Config ---------\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    r=16,\n","    bias=\"none\",\n","    target_modules=\"all-linear\",\n","    task_type=\"CAUSAL_LM\",\n","    modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",")\n","\n","# --------- SFT Configuration ---------\n","args = SFTConfig(\n","    output_dir=\"Gemma_3_Idiom_hypermoe_VL\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    gradient_checkpointing=True,\n","    optim=\"adamw_torch_fused\",\n","    logging_steps=10,\n","    save_strategy=\"steps\",\n","    save_steps=20,\n","    learning_rate=2e-4,\n","    bf16=True,\n","    max_grad_norm=0.3,\n","    warmup_ratio=0.03,\n","    lr_scheduler_type=\"constant\",\n","    push_to_hub=True,\n","    report_to=\"wandb\",\n","    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n","    dataset_text_field=\"\",\n","    dataset_kwargs={\"skip_prepare_dataset\": True},\n",")\n","args.remove_unused_columns = False\n","\n","print(\"++++++connecting to wandb++++++++++\")\n","wandb.init(\n","    project=\"Gemma_3_Idiom_VL\",\n","    name=\"Gemma_3_Idiom_VL\",\n","    config=args,\n",")\n","\n","# --------- Data Collator for SFTTrainer ---------\n","def collate_fn(examples):\n","    texts, images = [], []\n","    for example in examples:\n","        image_inputs = process_vision_info(example[\"messages\"])\n","        text = processor.apply_chat_template(example[\"messages\"], add_generation_prompt=False, tokenize=False)\n","        texts.append(text.strip())\n","        images.append(image_inputs)\n","    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n","    labels = batch[\"input_ids\"].clone()\n","\n","    # Mask pad tokens and image tokens\n","    image_token_id = processor.tokenizer.convert_tokens_to_ids(processor.tokenizer.special_tokens_map.get(\"boi_token\", \"\"))\n","    labels[labels == processor.tokenizer.pad_token_id] = -100\n","    if image_token_id != \"\":\n","        labels[labels == image_token_id] = -100\n","    labels[labels == 262144] = -100  # Just in case\n","\n","    batch[\"labels\"] = labels\n","    return batch\n","\n","# --------- Trainer initialization ---------\n","trainer = SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    peft_config=peft_config,\n","    processing_class=processor,\n","    data_collator=collate_fn,\n",")\n","\n","# Move moe modules to device and set train mode\n","moe_modules.to(\"cuda\")\n","moe_modules.train()\n","for p in moe_modules.parameters():\n","    p.requires_grad = True\n","\n","print(\"++++++Starting the training++++++++++\")\n","trainer.train()\n","\n","print(\"++++++Saving the Model++++++++++\")\n","trainer.save_model(args.output_dir)"],"metadata":{"id":"0DMRSZ2MdSur"},"id":"0DMRSZ2MdSur","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}