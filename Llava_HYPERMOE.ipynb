{"cells":[{"cell_type":"code","source":["from datasets import load_dataset, Dataset, concatenate_datasets\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from PIL import Image\n","from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig, AutoTokenizer\n","from peft import LoraConfig\n","from trl import SFTConfig, SFTTrainer\n","import wandb\n","\n","print(\"++++++Reading the Dataset++++++++++\")\n","dataset = load_dataset(\"Anonymous/Final_idiom_all\", split='train')\n","\n","from huggingface_hub import login\n","login(token='anonymous_hjhijpjiovvugviipjpjjm')\n","\n","system_message = '''You are an polyglot, who are having exceptional linguistic and cultural domain knowledge. Also, you are an native speaker of hindi, bengali and thai.'''\n","\n","def format_data(sample):\n","    return {\n","        \"messages\":[\n","            {\n","                \"role\": \"system\",\n","                \"content\": [\n","                    {\"type\": \"text\", \"text\": system_message},\n","                ],\n","            },\n","            {\n","                \"role\": \"user\",\n","                \"content\": [\n","                    {\"type\": \"text\", \"text\": sample['Actual idiom']},\n","                    {\"type\": \"image\", \"image\": sample[\"image\"]},\n","                ],\n","            },\n","            {\n","                \"role\": \"assistant\",\n","                \"content\": [\n","                    {\"type\": \"text\", \"text\": sample[\"Descriptive Meaning(Human Annotation)\"]}\n","                ],\n","            },\n","        ],\n","    }\n","\n","def process_vision_info(messages):\n","    image_inputs = []\n","    for msg in messages:\n","        content = msg.get(\"content\", [])\n","        if not isinstance(content, list):\n","            content = [content]\n","        for element in content:\n","            if isinstance(element, dict) and (\"image\" in element or element.get(\"type\") == \"image\"):\n","                if \"image\" in element:\n","                    image = element[\"image\"]\n","                else:\n","                    image = element\n","                image_inputs.append(image.convert(\"RGB\"))\n","    return image_inputs\n","\n","print(\"++++++Seperating the Dataset on Lingual Basis++++++++++\")\n","dataset_hindi = dataset.select(range(0,1277))\n","dataset_thai = dataset.select(range(1382,3133))\n","bengali_indices = list(range(1277,1382)) + list(range(3133,3533))\n","dataset_bengali = dataset.select(bengali_indices)\n","\n","def split_dataset(dataset1):\n","    train_testvalid = dataset1.train_test_split(test_size=0.3, seed=42)\n","    train_dataset = train_testvalid['train']\n","    temp_dataset = train_testvalid['test']\n","    val_test = temp_dataset.train_test_split(test_size=2/3, seed=42)\n","    val_dataset = val_test['train']\n","    test_dataset = val_test['test']\n","    return train_dataset, val_dataset, test_dataset\n","\n","print(\"++++++Splitting the Dataset and Merging++++++++++\")\n","train_dataset_hindi, val_dataset_hindi, test_dataset_hindi = split_dataset(dataset_hindi)\n","train_dataset_thai, val_dataset_thai, test_dataset_thai = split_dataset(dataset_thai)\n","train_dataset_bengali, val_dataset_bengali, test_dataset_bengali = split_dataset(dataset_bengali)\n","\n","train_dataset_final = concatenate_datasets([train_dataset_hindi, train_dataset_thai, train_dataset_bengali])\n","val_dataset_final = concatenate_datasets([val_dataset_hindi, val_dataset_thai, val_dataset_bengali])\n","test_dataset_final = concatenate_datasets([test_dataset_hindi, test_dataset_thai, test_dataset_bengali])\n","\n","print(len(train_dataset_final), len(val_dataset_final), len(test_dataset_final))\n","\n","print(\"++++++Converting the Dataset to JSON format++++++++++\")\n","train_dataset = [format_data(sample) for sample in train_dataset_final]\n","eval_dataset = [format_data(sample) for sample in val_dataset_final]\n","test_dataset = [format_data(sample) for sample in test_dataset_final]\n","print(train_dataset[2000])\n","\n","print(\"+++++++++++Loading Model+++++++++++\")\n","model_id = \"llava-hf/llava-1.5-7b-hf\"\n","\n","if torch.cuda.get_device_capability()[0] < 8:\n","    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n","\n","model_kwargs = dict(\n","    attn_implementation=\"flash_attention_2\",\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n",")\n","\n","model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n","    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",")\n","\n","vision_tokenizer = AutoTokenizer.from_pretrained(\n","    model_id,\n","    extra_special_tokens={\"image_token\": \"<image>\", \"boi_token\": \"<image_start>\", \"eoi_token\": \"<image_end>\"}\n",")\n","model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n","processor = AutoProcessor.from_pretrained(model_id)\n","\n","print(\"++++++Configuring LoRA and peft++++++++++\")\n","peft_config = LoraConfig(\n","    lora_alpha=16,\n","    lora_dropout=0.05,\n","    r=16,\n","    bias=\"none\",\n","    target_modules=\"all-linear\",\n","    task_type=\"CAUSAL_LM\",\n","    modules_to_save=[\n","        \"lm_head\",\n","        \"embed_tokens\",\n","    ],\n",")\n","\n","args = SFTConfig(\n","    output_dir=\"Hypermoe_Llava_Idiom_VL\",\n","    num_train_epochs=1,\n","    per_device_train_batch_size=1,\n","    gradient_accumulation_steps=4,\n","    gradient_checkpointing=True,\n","    optim=\"adamw_torch_fused\",\n","    logging_steps=5,\n","    save_strategy=\"epoch\",\n","    learning_rate=2e-4,\n","    bf16=True,\n","    max_grad_norm=0.3,\n","    warmup_ratio=0.03,\n","    lr_scheduler_type=\"constant\",\n","    push_to_hub=True,\n","    report_to=\"wandb\",\n","    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n","    dataset_text_field=\"\",\n","    dataset_kwargs={\"skip_prepare_dataset\": True},\n",")\n","args.remove_unused_columns = False\n","\n","print(\"++++++connecting to wandb++++++++++\")\n","wandb.init(\n","    project=\"Llava_Idiom_VL\",\n","    name=\"Llava_Idiom_VL\",\n","    config=args,\n",")\n","\n","# HyperMoE implementation\n","class HyperMoE(nn.Module):\n","    def __init__(self, embed_dim, num_experts=4, hidden_dim=1024):\n","        super().__init__()\n","        self.experts = nn.ModuleList([\n","            nn.Sequential(\n","                nn.Linear(embed_dim, hidden_dim),\n","                nn.ReLU(),\n","                nn.Linear(hidden_dim, embed_dim)\n","            ) for _ in range(num_experts)\n","        ])\n","        self.gate = nn.Linear(embed_dim, num_experts)\n","\n","    def forward(self, x):\n","        weights = F.softmax(self.gate(x), dim=-1)  # (B, num_experts)\n","        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)  # (B, num_experts, embed_dim)\n","        expert_mean = expert_outputs.mean(dim=1, keepdim=True)  # (B, 1, embed_dim)\n","        enhanced = expert_outputs + 0.1 * (expert_mean - expert_outputs)  # (B, num_experts, embed_dim)\n","        fused = torch.einsum(\"be,bed->bd\", weights, enhanced)  # (B, embed_dim)\n","        return fused\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","embed_dim = model.config.hidden_size if hasattr(model.config, \"hidden_size\") else model.config.text_config.hidden_size\n","num_moe = 3\n","moe_modules = nn.ModuleList([HyperMoE(embed_dim) for _ in range(num_moe)]).to(device)\n","\n","def multi_model_hypermoe_fusion(features: torch.Tensor) -> torch.Tensor:\n","    fused_outputs = [moe(features) for moe in moe_modules]\n","    stacked = torch.stack(fused_outputs, dim=1)\n","    final_fusion = stacked.mean(dim=1)\n","    return final_fusion\n","\n","def model_hook(module, inputs, outputs):\n","    if hasattr(outputs, \"last_hidden_state\"):\n","        h = outputs.last_hidden_state\n","    elif isinstance(outputs, tuple) and isinstance(outputs[0], torch.Tensor):\n","        h = outputs[0]\n","    else:\n","        return outputs\n","    cls = h[:, 0, :]\n","    fused = multi_model_hypermoe_fusion(cls)\n","    h[:, 0, :] = fused\n","    return outputs.__class__(**{**outputs.__dict__, \"last_hidden_state\": h})\n","\n","model.base_model.register_forward_hook(model_hook)\n","\n","def connector_hook(module, inputs, output):\n","    if isinstance(output, torch.Tensor) and output.dim() == 3:\n","        image_features = output\n","        cls_image_embed = image_features[:, 0, :]\n","        fused_image_embed = multi_model_hypermoe_fusion(cls_image_embed)\n","        image_features[:, 0, :] = fused_image_embed\n","        return image_features\n","    return output\n","\n","from transformers.models.idefics3.modeling_idefics3 import Idefics3VisionTransformer\n","\n","vision_module = None\n","for m in model.base_model.modules():\n","    if m.__class__.__name__ == \"Idefics3VisionTransformer\":\n","        vision_module = m\n","        break\n","\n","if vision_module is not None:\n","    vision_module.register_forward_hook(connector_hook)\n","\n","def collate_fn(examples):\n","    texts = []\n","    images = []\n","    for example in examples:\n","        image_inputs = process_vision_info(example[\"messages\"])\n","        text = processor.apply_chat_template(\n","            example[\"messages\"], add_generation_prompt=False, tokenize=False\n","        )\n","        texts.append(text.strip())\n","        images.append(image_inputs)\n","    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n","    labels = batch[\"input_ids\"].clone()\n","    image_token_id = [\n","        vision_tokenizer.convert_tokens_to_ids(\n","            vision_tokenizer.special_tokens_map[\"boi_token\"]\n","        )\n","    ]\n","    labels[labels == processor.tokenizer.pad_token_id] = -100\n","    for img_tok in image_token_id:\n","        labels[labels == img_tok] = -100\n","    labels[labels == 262144] = -100\n","    batch[\"labels\"] = labels\n","    return batch\n","\n","train_dataset = train_dataset\n","eval_dataset = eval_dataset\n","\n","# Attach moe_modules as attribute of model so params get optimized by SFTTrainer\n","model.moe_modules = moe_modules\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    args=args,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset,\n","    data_collator=collate_fn,\n","    peft_config=peft_config,\n","    processing_class=processor,\n","    # do not pass optimizer manually\n",")\n","\n","model.to(device)\n","moe_modules.to(device)\n","model.train()\n","moe_modules.train()\n","\n","print(\"++++++Starting the training++++++++++\")\n","trainer.train()\n","print(\"++++++Saving the Model++++++++++\")\n","trainer.save_model(args.output_dir)"],"metadata":{"id":"rVpyRacYdx55"},"id":"rVpyRacYdx55","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}